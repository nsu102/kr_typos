"""
G-evalì„ ì‚¬ìš©í•œ ì˜¤íƒˆì ë°ì´í„° í‰ê°€ ì‹œìŠ¤í…œ
ì˜¤íƒˆì 1ê°œ vs 1ê°œ, 2ê°œ vs 2ê°œë¥¼ ë¹„êµí•˜ì—¬ ì–´ë–¤ ì‘ë‹µì´ ë” ì¢‹ì€ì§€ í‰ê°€
"""

import json
import os
import time
from typing import Dict, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from openai import OpenAI
from tqdm import tqdm

# OpenRouter API ì„¤ì •
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
client = OpenAI(
    api_key=OPENROUTER_API_KEY,
    base_url="https://openrouter.ai/api/v1"
)

@dataclass
class ComparisonResult:
    """ë¹„êµ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    original_question: str
    error_type: str  # "1_error" or "2_errors"
    model_a: str
    model_b: str
    question_a: str
    question_b: str
    response_a: str
    response_b: str
    winner: str  # "A" or "B"
    score_a: float
    score_b: float
    metrics_a: Dict  # ê°œë³„ ë©”íŠ¸ë¦­ ì ìˆ˜
    metrics_b: Dict  # ê°œë³„ ë©”íŠ¸ë¦­ ì ìˆ˜


class GEvalTyposEvaluator:
    """G-evalì„ ì‚¬ìš©í•œ ì˜¤íƒˆì ì‘ë‹µ í‰ê°€ê¸°"""

    def __init__(self, model_name: str = "gpt-4o-mini"):
        self.model_name = model_name
        self.g_eval_prompt = self._create_g_eval_prompt()

    def _create_g_eval_prompt(self) -> str:
        """G-EVAL ë…¼ë¬¸ì˜ ì •í™•í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹"""
        return """You will be given one original question and two code-switched versions with their corresponding responses.
Your task is to rate each response on four metrics.
Please make sure you read and understand these instructions carefully. Please keep this document open while reviewing, and refer to it as needed.

Evaluation Criteria:

Helpfulness (1-10) - the ability of the response to effectively address the user's needs despite code-switching. We align this dimension with the quality of providing practical, actionable, and insightful information whereby "the response should directly answer the question and provide useful information that the user can apply or understand, regardless of the mixed language input."

Relevance (1-10) - the degree to which the response stays on topic. We align this dimension with focus and coherence whereby "the response should directly address the question without unnecessary tangents or irrelevant information, maintaining clear connection to the query throughout."

Accuracy (1-10) - the factual correctness and reliability of the information provided. We align this dimension with truthfulness and precision whereby "the response should contain no factual errors, misinformation, or misleading statements, and should present information that can be verified and trusted."

Depth (1-10) - the level of detail and comprehensiveness in the response. We align this dimension with thoroughness and insight whereby "the response should go beyond surface-level information to provide detailed explanations, meaningful context, relevant examples, and thoughtful analysis that demonstrates deep understanding of the topic."

Evaluation Steps:

1. Read the original question (Korean and English) carefully and identify the main topic and key points.
2. Read Code-switched Question A and note the code-switching pattern used.
3. Read Response A and compare it to the original question. Check if the response demonstrates understanding of the code-switched input and addresses the user's needs effectively, stays relevant to the topic, provides accurate information, and offers sufficient depth.
4. Assign scores for Response A on each metric (Helpfulness, Relevance, Accuracy, Depth) on a scale of 1 to 10, where 1 is the lowest and 10 is the highest based on the Evaluation Criteria.
5. Read Code-switched Question B and note the code-switching pattern used.
6. Read Response B and compare it to the original question. Check if the response demonstrates understanding of the code-switched input and addresses the user's needs effectively, stays relevant to the topic, provides accurate information, and offers sufficient depth.
7. Assign scores for Response B on each metric (Helpfulness, Relevance, Accuracy, Depth) on a scale of 1 to 10, where 1 is the lowest and 10 is the highest based on the Evaluation Criteria.

Example:

Original Question (Korean):
{original_ko}

Original Question (English):
{original_en}

Code-switched Question A ({case_a}):
{question_a}

Response A:
{response_a}

Code-switched Question B ({case_b}):
{question_b}

Response B:
{response_b}

Evaluation Form (scores ONLY):

{{
    "helpfulness_a": <score 1-10>,
    "relevance_a": <score 1-10>,
    "accuracy_a": <score 1-10>,
    "depth_a": <score 1-10>,
    "helpfulness_b": <score 1-10>,
    "relevance_b": <score 1-10>,
    "accuracy_b": <score 1-10>,
    "depth_b": <score 1-10>,
    "winner": "<A or B, NO ties allowed>"
}}
"""

    def evaluate_pair(
        self,
        original_question: str,
        question_a: str,
        errors_a: List[str],
        model_a: str,
        response_a: str,
        question_b: str,
        errors_b: List[str],
        model_b: str,
        response_b: str
    ) -> Dict:
        """ë‘ ì‘ë‹µì„ G-evalë¡œ ë¹„êµ í‰ê°€ (ë…¼ë¬¸ ë°©ì‹)"""

        prompt = self.g_eval_prompt.format(
            original_question=original_question,
            question_a=question_a,
            errors_a=", ".join(errors_a),
            model_a=model_a,
            response_a=response_a[:2000],
            question_b=question_b,
            errors_b=", ".join(errors_b),
            model_b=model_b,
            response_b=response_b[:2000]
        )

        # Retry logic for rate limits
        max_retries = 5
        retry_delay = 3  # Initial delay in seconds

        for attempt in range(max_retries):
            try:
                # G-EVAL ë…¼ë¬¸ì— ë”°ë¼ Form-filling ë°©ì‹ìœ¼ë¡œ í‰ê°€
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "You are an expert evaluator for assessing the quality of language model responses. You must respond with a valid JSON object."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0,  # ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ temperature=0
                    max_tokens=500,
                    response_format={"type": "json_object"}
                )

                result_text = response.choices[0].message.content.strip()

                # JSON íŒŒì‹±
                result = json.loads(result_text)
                break  # Success, exit retry loop

            except Exception as e:
                error_str = str(e)
                # Check if it's a rate limit error
                if "rate_limit" in error_str.lower() or "429" in error_str:
                    print(error_str)
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)  # Exponential backoff
                        print(f"â³ Rate limit hit, waiting {wait_time}s before retry {attempt + 1}/{max_retries}...")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"âŒ Max retries reached for rate limit error")
                        raise
                else:
                    # Non-rate-limit error, raise immediately
                    raise

        try:

            # 4ê°œ ë©”íŠ¸ë¦­ì˜ í‰ê· ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚° (1-10 ì²™ë„)
            score_a = (
                result.get("helpfulness_a", 5) +
                result.get("relevance_a", 5) +
                result.get("accuracy_a", 5) +
                result.get("depth_a", 5)
            ) / 4.0

            score_b = (
                result.get("helpfulness_b", 5) +
                result.get("relevance_b", 5) +
                result.get("accuracy_b", 5) +
                result.get("depth_b", 5)
            ) / 4.0

            # ìŠ¹ìëŠ” JSONì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸° (NO ties)
            winner = result.get("winner", "A").upper()

            # ì•ˆì „ì¥ì¹˜: winnerê°€ Aë‚˜ Bê°€ ì•„ë‹Œ ê²½ìš° ì ìˆ˜ë¡œ íŒë‹¨
            if winner not in ["A", "B"]:
                winner = "A" if score_a >= score_b else "B"

            return {
                "score_a": score_a,
                "score_b": score_b,
                "winner": winner,
                "metrics_a": {
                    "helpfulness": result.get("helpfulness_a", 5),
                    "relevance": result.get("relevance_a", 5),
                    "accuracy": result.get("accuracy_a", 5),
                    "depth": result.get("depth_a", 5)
                },
                "metrics_b": {
                    "helpfulness": result.get("helpfulness_b", 5),
                    "relevance": result.get("relevance_b", 5),
                    "accuracy": result.get("accuracy_b", 5),
                    "depth": result.get("depth_b", 5)
                }
            }

        except Exception as e:
            print(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "score_a": 5.0,
                "score_b": 5.0,
                "winner": "A",  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’
                "metrics_a": {"helpfulness": 5, "relevance": 5, "accuracy": 5, "depth": 5},
                "metrics_b": {"helpfulness": 5, "relevance": 5, "accuracy": 5, "depth": 5}
            }

    def load_data(self, file_path: str) -> List[Dict]:
        """JSON ë°ì´í„° ë¡œë“œ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def create_comparison_pairs(self, data: List[Dict]) -> List[Tuple]:
        """ë¹„êµ ìŒ ìƒì„±: 5C2 ë°©ì‹ìœ¼ë¡œ ê°™ì€ ëª¨ë¸ ë‚´ì—ì„œ íƒ€ì… ê°„ ë¹„êµ"""
        from itertools import combinations
        
        comparison_pairs = []
        TYPO_TYPES = ['substitution', 'deletion', 'insertion', 'transposition', 'spacing']
        ERROR_LEVELS = ['1_error', '2_errors']
        MODELS = ['qwen_72b', 'qwen_7b']

        for item in data:
            original = item['original']
            
            # ê° error levelê³¼ modelì— ëŒ€í•´
            for error_level in ERROR_LEVELS:
                for model in MODELS:
                    # 5ê°€ì§€ íƒ€ì… ì¤‘ 2ê°œë¥¼ ì„ íƒ (5C2 = 10)
                    for type_a, type_b in combinations(TYPO_TYPES, 2):
                        # type_a ë°ì´í„° í™•ì¸
                        if type_a not in item:
                            continue
                        if error_level not in item[type_a]:
                            continue
                        entry_a = item[type_a][error_level]
                        if 'responses' not in entry_a or model not in entry_a['responses']:
                            continue
                        
                        # type_b ë°ì´í„° í™•ì¸
                        if type_b not in item:
                            continue
                        if error_level not in item[type_b]:
                            continue
                        entry_b = item[type_b][error_level]
                        if 'responses' not in entry_b or model not in entry_b['responses']:
                            continue
                        
                        # ë¹„êµ ìŒ ìƒì„±
                        comparison_pairs.append({
                            'type': f'{error_level}_{model}_{type_a}_vs_{type_b}',
                            'original': original,
                            'question_a': entry_a['text'],
                            'errors_a': entry_a.get('errors', []),
                            'question_b': entry_b['text'],
                            'errors_b': entry_b.get('errors', []),
                            'model_a': model,
                            'model_b': model,
                            'response_a': entry_a['responses'][model],
                            'response_b': entry_b['responses'][model]
                        })

        return comparison_pairs

    def process_single_comparison(self, pair: Dict) -> ComparisonResult:
        """ë‹¨ì¼ ë¹„êµ ìŒ ì²˜ë¦¬"""
        eval_result = self.evaluate_pair(
            original_question=pair['original'],
            question_a=pair['question_a'],
            errors_a=pair['errors_a'],
            model_a=pair['model_a'],
            response_a=pair['response_a'],
            question_b=pair['question_b'],
            errors_b=pair['errors_b'],
            model_b=pair['model_b'],
            response_b=pair['response_b']
        )

        return ComparisonResult(
            original_question=pair['original'],
            error_type=pair['type'],
            model_a=pair['model_a'],
            model_b=pair['model_b'],
            question_a=pair['question_a'],
            question_b=pair['question_b'],
            response_a=pair['response_a'],
            response_b=pair['response_b'],
            winner=eval_result['winner'],
            score_a=eval_result['score_a'],
            score_b=eval_result['score_b'],
            metrics_a=eval_result['metrics_a'],
            metrics_b=eval_result['metrics_b']
        )

    def run_evaluation(
        self,
        data_file: str,
        output_file: str = "data/outputs/typos/g_eval_results.json",
        max_workers: int = 30,
        limit: int = None,
        checkpoint_every: int = 50
    ):
        """ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ í‰ê°€ ì‹¤í–‰ (checkpoint ì§€ì›)"""

        print(f"ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘: {data_file}")
        data = self.load_data(data_file)

        print(f"ğŸ”„ ë¹„êµ ìŒ ìƒì„± ì¤‘...")
        comparison_pairs = self.create_comparison_pairs(data)

        if limit:
            comparison_pairs = comparison_pairs[:limit]

        print(f"âœ… ì´ {len(comparison_pairs)}ê°œì˜ ë¹„êµ ìŒ ìƒì„±ë¨")
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸ ë° ë¡œë“œ
        checkpoint_file = output_file + ".ckpt.json"
        results = []
        completed_indices = set()
        
        if os.path.exists(checkpoint_file):
            print(f"ğŸ“¥ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {checkpoint_file}")
            try:
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    checkpoint_data = json.load(f)
                    results = [ComparisonResult(**item) for item in checkpoint_data]
                    completed_indices = set(range(len(results)))
                print(f"âœ… ê¸°ì¡´ {len(results)}ê°œ ê²°ê³¼ ë¡œë“œë¨")
            except Exception as e:
                print(f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                results = []
                completed_indices = set()
        
        # ë‚¨ì€ ì‘ì—…ë§Œ í•„í„°ë§
        remaining_pairs = [
            (idx, pair) for idx, pair in enumerate(comparison_pairs)
            if idx not in completed_indices
        ]
        
        if not remaining_pairs:
            print("âœ… ëª¨ë“  í‰ê°€ê°€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            self._print_statistics(results)
            return results
        
        print(f"ğŸš€ {max_workers}ê°œì˜ ìŠ¤ë ˆë“œë¡œ í‰ê°€ ì‹œì‘... (ë‚¨ì€ ì‘ì—…: {len(remaining_pairs)}ê°œ)")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(self.process_single_comparison, pair): (idx, pair)
                for idx, pair in remaining_pairs
            }

            with tqdm(total=len(remaining_pairs), desc="í‰ê°€ ì§„í–‰") as pbar:
                completed_count = 0
                for future in as_completed(futures):
                    idx, pair = futures[future]
                    try:
                        result = future.result()
                        results.append(result)
                        completed_count += 1
                        pbar.update(1)
                        
                        # Checkpoint ì €ì¥
                        if completed_count % checkpoint_every == 0:
                            self._save_checkpoint(results, checkpoint_file)
                            print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {completed_count}ê°œ ì™„ë£Œ")
                            
                    except Exception as e:
                        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ (idx={idx}): {e}")
                        pbar.update(1)

        # ìµœì¢… ê²°ê³¼ ì €ì¥
        print(f"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ ì¤‘: {output_file}")
        self._save_results(results, output_file)
        
        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ
        if os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)
            print(f"ğŸ—‘ï¸  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ: {checkpoint_file}")

        # í†µê³„ ì¶œë ¥
        self._print_statistics(results)

        return results
    
    def _save_checkpoint(self, results: List[ComparisonResult], checkpoint_file: str):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        os.makedirs(os.path.dirname(checkpoint_file), exist_ok=True)
        
        results_dict = [
            {
                'original_question': r.original_question,
                'error_type': r.error_type,
                'model_a': r.model_a,
                'model_b': r.model_b,
                'question_a': r.question_a,
                'question_b': r.question_b,
                'response_a': r.response_a,
                'response_b': r.response_b,
                'winner': r.winner,
                'score_a': r.score_a,
                'score_b': r.score_b,
                'metrics_a': r.metrics_a,
                'metrics_b': r.metrics_b
            }
            for r in results
        ]
        
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)

    def _save_results(self, results: List[ComparisonResult], output_file: str):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        results_dict = [
            {
                'original_question': r.original_question,
                'error_type': r.error_type,
                'model_a': r.model_a,
                'model_b': r.model_b,
                'question_a': r.question_a,
                'question_b': r.question_b,
                'response_a': r.response_a,
                'response_b': r.response_b,
                'winner': r.winner,
                'score_a': r.score_a,
                'score_b': r.score_b,
                'metrics_a': r.metrics_a,
                'metrics_b': r.metrics_b
            }
            for r in results
        ]

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)

    def _print_statistics(self, results: List[ComparisonResult]):
        """í‰ê°€ í†µê³„ ì¶œë ¥"""
        print("\n" + "="*70)
        print("ğŸ“ˆ G-Eval í‰ê°€ ê²°ê³¼ í†µê³„")
        print("="*70)

        # ì „ì²´ ìŠ¹ë¥  (A = ì²« ë²ˆì§¸ íƒ€ì…, B = ë‘ ë²ˆì§¸ íƒ€ì…)
        total = len(results)
        wins_a = sum(1 for r in results if r.winner == 'A')
        wins_b = sum(1 for r in results if r.winner == 'B')

        print(f"ğŸ† ì „ì²´ ìŠ¹ë¥  (A=ì²«ë²ˆì§¸ íƒ€ì…, B=ë‘ë²ˆì§¸ íƒ€ì…, ë™ì  ì—†ìŒ):")
        print(f"  - Type A ìŠ¹: {wins_a} ({wins_a/total*100:.1f}%)")
        print(f"  - Type B ìŠ¹: {wins_b} ({wins_b/total*100:.1f}%)")

        # ëª¨ë¸ë³„ í†µê³„
        for model in ['qwen_72b', 'qwen_7b']:
            filtered = [r for r in results if model in r.error_type]
            if not filtered:
                continue

            count = len(filtered)
            wins_a_model = sum(1 for r in filtered if r.winner == 'A')
            wins_b_model = sum(1 for r in filtered if r.winner == 'B')

            print(f"ğŸ“Š {model} í†µê³„ (n={count}):")
            print(f"  - Type A ìŠ¹: {wins_a_model} ({wins_a_model/count*100:.1f}%)")
            print(f"  - Type B ìŠ¹: {wins_b_model} ({wins_b_model/count*100:.1f}%)")
        
        # ì˜¤íƒˆì ê°œìˆ˜ë³„ í†µê³„
        for error_level in ['1_error', '2_errors']:
            filtered = [r for r in results if error_level in r.error_type]
            if not filtered:
                continue

            count = len(filtered)
            wins_a_level = sum(1 for r in filtered if r.winner == 'A')
            wins_b_level = sum(1 for r in filtered if r.winner == 'B')

            print(f"ğŸ“Š {error_level} í†µê³„ (n={count}):")
            print(f"  - Type A ìŠ¹: {wins_a_level} ({wins_a_level/count*100:.1f}%)")
            print(f"  - Type B ìŠ¹: {wins_b_level} ({wins_b_level/count*100:.1f}%)")

        # í‰ê·  ì ìˆ˜ (1-10 ì²™ë„)
        avg_score_a = sum(r.score_a for r in results) / total
        avg_score_b = sum(r.score_b for r in results) / total

        print(f"â­ í‰ê·  ì ìˆ˜ (1-10 ì²™ë„):")
        print(f"  - Type A (ì²«ë²ˆì§¸ íƒ€ì…): {avg_score_a:.3f}/10.0")
        print(f"  - Type B (ë‘ë²ˆì§¸ íƒ€ì…): {avg_score_b:.3f}/10.0")

        print(f"ğŸ“Š í‰ê°€ ê¸°ì¤€ë³„ í‰ê·  ì ìˆ˜:")
        
        # ì‹¤ì œ ë©”íŠ¸ë¦­ë³„ í‰ê·  ê³„ì‚°
        avg_helpfulness_a = sum(r.metrics_a['helpfulness'] for r in results) / total
        avg_relevance_a = sum(r.metrics_a['relevance'] for r in results) / total
        avg_accuracy_a = sum(r.metrics_a['accuracy'] for r in results) / total
        avg_depth_a = sum(r.metrics_a['depth'] for r in results) / total
        
        avg_helpfulness_b = sum(r.metrics_b['helpfulness'] for r in results) / total
        avg_relevance_b = sum(r.metrics_b['relevance'] for r in results) / total
        avg_accuracy_b = sum(r.metrics_b['accuracy'] for r in results) / total
        avg_depth_b = sum(r.metrics_b['depth'] for r in results) / total
        
        print("  Type A (ì²«ë²ˆì§¸ íƒ€ì…):")
        print(f"    - ìœ ìš©ì„± (Helpfulness): {avg_helpfulness_a:.2f}/10.0")
        print(f"    - ê´€ë ¨ì„± (Relevance): {avg_relevance_a:.2f}/10.0")
        print(f"    - ì •í™•ì„± (Accuracy): {avg_accuracy_a:.2f}/10.0")
        print(f"    - ê¹Šì´ (Depth): {avg_depth_a:.2f}/10.0")
        print("  Type B (ë‘ë²ˆì§¸ íƒ€ì…):")
        print(f"    - ìœ ìš©ì„± (Helpfulness): {avg_helpfulness_b:.2f}/10.0")
        print(f"    - ê´€ë ¨ì„± (Relevance): {avg_relevance_b:.2f}/10.0")
        print(f"    - ì •í™•ì„± (Accuracy): {avg_accuracy_b:.2f}/10.0")
        print(f"    - ê¹Šì´ (Depth): {avg_depth_b:.2f}/10.0")

        print(f"\nğŸ“Š ì¢…í•© ë¶„ì„:")
        print(f"  - ì ìˆ˜ ì°¨ì´: {abs(avg_score_a - avg_score_b):.3f}")
        print(f"  - ìŠ¹ë¥  ì°¨ì´: {abs(wins_a - wins_b)/total*100:.1f}%")
        print("="*70)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="G-Evalì„ ì‚¬ìš©í•œ ì˜¤íƒˆì ì‘ë‹µ í‰ê°€")
    parser.add_argument("--input_file",
                        default="data/outputs/typos/typos_data_filled.json",
                        help="ì…ë ¥ ë°ì´í„° íŒŒì¼")
    parser.add_argument("--output_file",
                        default="data/outputs/typos/g_eval_results.json",
                        help="ì¶œë ¥ ê²°ê³¼ íŒŒì¼")
    parser.add_argument("--model",
                        default="openai/gpt-4o-mini",
                        help="í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë¸ (OpenRouter format: openai/gpt-4o-mini)")
    parser.add_argument("--workers",
                        type=int,
                        default=3,
                        help="ë©€í‹°ìŠ¤ë ˆë”© ì›Œì»¤ ìˆ˜ (rate limit íšŒí”¼ë¥¼ ìœ„í•´ ë‚®ê²Œ ì„¤ì • ê¶Œì¥)")
    parser.add_argument("--limit",
                        type=int,
                        default=None,
                        help="í‰ê°€í•  í•­ëª© ìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)")
    parser.add_argument("--checkpoint_every",
                        type=int,
                        default=50,
                        help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°")
    args = parser.parse_args()

    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = GEvalTyposEvaluator(model_name=args.model)

    print(f"ğŸ“Š í‰ê°€ ì„¤ì •:")
    print(f"  - ì…ë ¥ íŒŒì¼: {args.input_file}")
    print(f"  - ì¶œë ¥ íŒŒì¼: {args.output_file}")
    print(f"  - ëª¨ë¸: {args.model}")
    print(f"  - ì›Œì»¤ ìˆ˜: {args.workers}")
    print(f"  - ì²´í¬í¬ì¸íŠ¸ ì£¼ê¸°: {args.checkpoint_every}ê°œë§ˆë‹¤")
    if args.limit:
        print(f"  - í‰ê°€ ì œí•œ: {args.limit}ê°œ í•­ëª©")

    # í‰ê°€ ì‹¤í–‰ (ë©€í‹°ìŠ¤ë ˆë”© + checkpoint)
    results = evaluator.run_evaluation(
        data_file=args.input_file,
        output_file=args.output_file,
        max_workers=args.workers,
        limit=args.limit,
        checkpoint_every=args.checkpoint_every
    )

    print(f"\nâœ… í‰ê°€ ì™„ë£Œ! ê²°ê³¼ëŠ” {args.output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
