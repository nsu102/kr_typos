"""
G-evalì„ ì‚¬ìš©í•œ ì½”ë“œ ìŠ¤ìœ„ì¹­ ë°ì´í„° í‰ê°€ ì‹œìŠ¤í…œ
5C2 ë°©ì‹ìœ¼ë¡œ Caseë“¤ë¼ë¦¬ ë¹„êµí•˜ì—¬ ì–´ë–¤ ì‘ë‹µì´ ë” ì¢‹ì€ì§€ í‰ê°€
"""

import json
import os
import time
from typing import Dict, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from openai import OpenAI
from tqdm import tqdm
from itertools import combinations

# OpenRouter API ì„¤ì •
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
client = OpenAI(
    api_key=OPENROUTER_API_KEY,
    base_url="https://openrouter.ai/api/v1"
)

@dataclass
class ComparisonResult:
    """ë¹„êµ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    original_question: str  # original_ko
    original_en: str
    case_a: str
    case_b: str
    model: str
    question_a: str
    question_b: str
    response_a: str
    response_b: str
    score_a: float
    score_b: float
    metrics_a: Dict  # ê°œë³„ ë©”íŠ¸ë¦­ ì ìˆ˜
    metrics_b: Dict  # ê°œë³„ ë©”íŠ¸ë¦­ ì ìˆ˜


class GEvalCodeSwitchingEvaluator:
    """G-evalì„ ì‚¬ìš©í•œ ì½”ë“œ ìŠ¤ìœ„ì¹­ ì‘ë‹µ í‰ê°€ê¸°"""

    def __init__(self, model_name: str = "gpt-4o-mini"):
        self.model_name = model_name
        self.g_eval_prompt = self._create_g_eval_prompt()

    def _create_g_eval_prompt(self) -> str:
        """G-EVAL exact prompt format (English)"""
        return """You are given one original question and two code-switching versions of the question, along with responses to each.
Your task is to rate each response according to four evaluation criteria.
Please read and understand these instructions carefully. Keep this document open while evaluating and refer to it as needed.

Evaluation Criteria:

Helpfulness (1-10) - The ability of the response to effectively meet the user's needs despite code-switching. Aligns with the quality of providing practical, actionable, and insightful information in that "the response should directly answer the question and provide useful information that the user can apply or understand, regardless of the mixed-language input."

Relevance (1-10) - How well the response stays on topic. Aligns with focus and coherence in that "the response should directly address the question without unnecessary digressions or irrelevant information, maintaining a clear connection to the question throughout."

Accuracy (1-10) - The factual correctness and reliability of the information provided. Aligns with truthfulness and precision in that "the response should not contain factual errors, misinformation, or misleading statements, and should present verifiable and trustworthy information."

Depth (1-10) - The level of detail and comprehensiveness in the response. Aligns with thoroughness and insight in that "the response should go beyond surface-level information to provide detailed explanations, meaningful context, relevant examples, and thoughtful analysis that demonstrates deep understanding of the topic."

Evaluation Steps:

1. Read the original question (in Korean and English) and identify the main topic and key points.
2. Read code-switching question A and note the code-switching pattern used.
3. Read response A and compare it to the original question. Check whether the response demonstrates understanding of the code-switching input, effectively meets the user's needs, maintains relevance to the topic, provides accurate information, and offers sufficient depth.
4. Assign a score from 1 to 10 for each evaluation criterion (Helpfulness, Relevance, Accuracy, Depth) for response A. 1 is the lowest, 10 is the highest.
5. Read code-switching question B and note the code-switching pattern used.
6. Read response B and compare it to the original question. Check whether the response demonstrates understanding of the code-switching input, effectively meets the user's needs, maintains relevance to the topic, provides accurate information, and offers sufficient depth.
7. Assign a score from 1 to 10 for each evaluation criterion (Helpfulness, Relevance, Accuracy, Depth) for response B. 1 is the lowest, 10 is the highest.

Example:

Original Question (Korean):
{original_ko}

Original Question (English):
{original_en}

Code-switching Question A ({case_a}):
{question_a}

Response A:
{response_a}

Code-switching Question B ({case_b}):
{question_b}

Response B:
{response_b}

Evaluation Form (scores only):

{{
    "helpfulness_a": <score 1-10>,
    "relevance_a": <score 1-10>,
    "accuracy_a": <score 1-10>,
    "depth_a": <score 1-10>,
    "helpfulness_b": <score 1-10>,
    "relevance_b": <score 1-10>,
    "accuracy_b": <score 1-10>,
    "depth_b": <score 1-10>
}}
"""

    def evaluate_pair(
        self,
        original_ko: str,
        original_en: str,
        case_a: str,
        question_a: str,
        model_a: str,
        response_a: str,
        case_b: str,
        question_b: str,
        model_b: str,
        response_b: str
    ) -> Dict:
        """ë‘ ì‘ë‹µì„ G-evalë¡œ ë¹„êµ í‰ê°€"""

        prompt = self.g_eval_prompt.format(
            original_ko=original_ko,
            original_en=original_en,
            case_a=case_a,
            question_a=question_a,
            model_a=model_a,
            response_a=response_a[:2000],
            case_b=case_b,
            question_b=question_b,
            model_b=model_b,
            response_b=response_b[:2000]
        )

        # Retry logic for rate limits
        max_retries = 5
        retry_delay = 3

        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì–¸ì–´ ëª¨ë¸ ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ í‰ê°€ìì…ë‹ˆë‹¤. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON ê°ì²´ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0,
                    max_tokens=500,
                    response_format={"type": "json_object"}
                )

                result_text = response.choices[0].message.content.strip()
                result = json.loads(result_text)
                break

            except Exception as e:
                error_str = str(e)
                if "rate_limit" in error_str.lower() or "429" in error_str:
                    if attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)
                        print(f"â³ Rate limit hit, waiting {wait_time}s before retry {attempt + 1}/{max_retries}...")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"âŒ Max retries reached for rate limit error")
                        raise
                else:
                    raise

        try:
            # 4ê°œ ë©”íŠ¸ë¦­ì˜ í‰ê· ìœ¼ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
            score_a = (
                result.get("helpfulness_a", 5) +
                result.get("relevance_a", 5) +
                result.get("accuracy_a", 5) +
                result.get("depth_a", 5)
            ) / 4.0

            score_b = (
                result.get("helpfulness_b", 5) +
                result.get("relevance_b", 5) +
                result.get("accuracy_b", 5) +
                result.get("depth_b", 5)
            ) / 4.0

            return {
                "score_a": score_a,
                "score_b": score_b,
                "metrics_a": {
                    "helpfulness": result.get("helpfulness_a", 5),
                    "relevance": result.get("relevance_a", 5),
                    "accuracy": result.get("accuracy_a", 5),
                    "depth": result.get("depth_a", 5)
                },
                "metrics_b": {
                    "helpfulness": result.get("helpfulness_b", 5),
                    "relevance": result.get("relevance_b", 5),
                    "accuracy": result.get("accuracy_b", 5),
                    "depth": result.get("depth_b", 5)
                }
            }

        except Exception as e:
            print(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "score_a": 5.0,
                "score_b": 5.0,
                "metrics_a": {"helpfulness": 5, "relevance": 5, "accuracy": 5, "depth": 5},
                "metrics_b": {"helpfulness": 5, "relevance": 5, "accuracy": 5, "depth": 5}
            }

    def load_data(self, file_path: str) -> List[Dict]:
        """JSON ë°ì´í„° ë¡œë“œ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def create_comparison_pairs(self, data: List[Dict]) -> List[Dict]:
        """ë¹„êµ ìŒ ìƒì„±: 5C2 ë°©ì‹ìœ¼ë¡œ Caseë“¤ë¼ë¦¬ ë¹„êµ"""
        comparison_pairs = []
        CASES = ['Case1', 'Case2', 'Case3', 'Case4', 'Case5']
        MODELS = ['qwen_72b', 'qwen_7b']

        for item in data:
            question_id = item['id']
            original_ko = item['original_ko']
            original_en = item['original_en']

            # ê° ëª¨ë¸ì— ëŒ€í•´
            for model in MODELS:
                # 5ê°œ Case ì¤‘ 2ê°œë¥¼ ì„ íƒ (5C2 = 10)
                for case_a, case_b in combinations(CASES, 2):
                    # Case A ë°ì´í„° í™•ì¸
                    if case_a not in item:
                        continue
                    entry_a = item[case_a]
                    if 'responses' not in entry_a or model not in entry_a['responses']:
                        continue

                    # Case B ë°ì´í„° í™•ì¸
                    if case_b not in item:
                        continue
                    entry_b = item[case_b]
                    if 'responses' not in entry_b or model not in entry_b['responses']:
                        continue

                    # ë¹„êµ ìŒ ìƒì„±
                    comparison_pairs.append({
                        'question_id': question_id,
                        'original_ko': original_ko,
                        'original_en': original_en,
                        'case_a': case_a,
                        'case_b': case_b,
                        'model': model,
                        'question_a': entry_a['text'],
                        'question_b': entry_b['text'],
                        'response_a': entry_a['responses'][model],
                        'response_b': entry_b['responses'][model]
                    })

        return comparison_pairs

    def process_single_comparison(self, pair: Dict) -> ComparisonResult:
        """ë‹¨ì¼ ë¹„êµ ìŒ ì²˜ë¦¬"""
        eval_result = self.evaluate_pair(
            original_ko=pair['original_ko'],
            original_en=pair['original_en'],
            case_a=pair['case_a'],
            question_a=pair['question_a'],
            model_a=pair['model'],
            response_a=pair['response_a'],
            case_b=pair['case_b'],
            question_b=pair['question_b'],
            model_b=pair['model'],
            response_b=pair['response_b']
        )

        return ComparisonResult(
            original_question=pair['original_ko'],
            original_en=pair['original_en'],
            case_a=pair['case_a'],
            case_b=pair['case_b'],
            model=pair['model'],
            question_a=pair['question_a'],
            question_b=pair['question_b'],
            response_a=pair['response_a'],
            response_b=pair['response_b'],
            score_a=eval_result['score_a'],
            score_b=eval_result['score_b'],
            metrics_a=eval_result['metrics_a'],
            metrics_b=eval_result['metrics_b']
        )

    def run_evaluation(
        self,
        data_file: str,
        output_file: str = "data/outputs/cs/g_eval_results.json",
        max_workers: int = 3,
        limit: int = None,
        checkpoint_every: int = 50
    ):
        """ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ í‰ê°€ ì‹¤í–‰ (checkpoint ì§€ì›)"""

        print(f"ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘: {data_file}")
        data = self.load_data(data_file)

        print(f"ğŸ”„ ë¹„êµ ìŒ ìƒì„± ì¤‘...")
        comparison_pairs = self.create_comparison_pairs(data)

        if limit:
            comparison_pairs = comparison_pairs[:limit]

        print(f"âœ… ì´ {len(comparison_pairs)}ê°œì˜ ë¹„êµ ìŒ ìƒì„±ë¨")

        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸ ë° ë¡œë“œ
        checkpoint_file = output_file + ".ckpt.json"
        results = []
        completed_indices = set()

        if os.path.exists(checkpoint_file):
            print(f"ğŸ“¥ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {checkpoint_file}")
            try:
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    checkpoint_data = json.load(f)
                    results = [ComparisonResult(**item) for item in checkpoint_data]
                    completed_indices = set(range(len(results)))
                print(f"âœ… ê¸°ì¡´ {len(results)}ê°œ ê²°ê³¼ ë¡œë“œë¨")
            except Exception as e:
                print(f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                results = []
                completed_indices = set()

        # ë‚¨ì€ ì‘ì—…ë§Œ í•„í„°ë§
        remaining_pairs = [
            (idx, pair) for idx, pair in enumerate(comparison_pairs)
            if idx not in completed_indices
        ]

        if not remaining_pairs:
            print("âœ… ëª¨ë“  í‰ê°€ê°€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            self._print_statistics(results)
            return results

        print(f"ğŸš€ {max_workers}ê°œì˜ ìŠ¤ë ˆë“œë¡œ í‰ê°€ ì‹œì‘... (ë‚¨ì€ ì‘ì—…: {len(remaining_pairs)}ê°œ)")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(self.process_single_comparison, pair): (idx, pair)
                for idx, pair in remaining_pairs
            }

            with tqdm(total=len(remaining_pairs), desc="í‰ê°€ ì§„í–‰") as pbar:
                completed_count = 0
                for future in as_completed(futures):
                    idx, pair = futures[future]
                    try:
                        result = future.result()
                        results.append(result)
                        completed_count += 1
                        pbar.update(1)

                        # Checkpoint ì €ì¥
                        if completed_count % checkpoint_every == 0:
                            self._save_checkpoint(results, checkpoint_file)
                            print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {completed_count}ê°œ ì™„ë£Œ")

                    except Exception as e:
                        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ (idx={idx}): {e}")
                        pbar.update(1)

        # ìµœì¢… ê²°ê³¼ ì €ì¥
        print(f"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥ ì¤‘: {output_file}")
        self._save_results(results, output_file)

        # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ
        if os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)
            print(f"ğŸ—‘ï¸  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì‚­ì œ: {checkpoint_file}")

        # í†µê³„ ì¶œë ¥
        self._print_statistics(results)

        return results

    def _save_checkpoint(self, results: List[ComparisonResult], checkpoint_file: str):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        os.makedirs(os.path.dirname(checkpoint_file), exist_ok=True)

        results_dict = [
            {
                'original_question': r.original_question,
                'original_en': r.original_en,
                'case_a': r.case_a,
                'case_b': r.case_b,
                'model': r.model,
                'question_a': r.question_a,
                'question_b': r.question_b,
                'response_a': r.response_a,
                'response_b': r.response_b,
                'score_a': r.score_a,
                'score_b': r.score_b,
                'metrics_a': r.metrics_a,
                'metrics_b': r.metrics_b
            }
            for r in results
        ]

        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)

    def _save_results(self, results: List[ComparisonResult], output_file: str):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        results_dict = [
            {
                'original_question': r.original_question,
                'original_en': r.original_en,
                'case_a': r.case_a,
                'case_b': r.case_b,
                'model': r.model,
                'question_a': r.question_a,
                'question_b': r.question_b,
                'response_a': r.response_a,
                'response_b': r.response_b,
                'score_a': r.score_a,
                'score_b': r.score_b,
                'metrics_a': r.metrics_a,
                'metrics_b': r.metrics_b
            }
            for r in results
        ]

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)

    def _print_statistics(self, results: List[ComparisonResult]):
        """í‰ê°€ í†µê³„ ì¶œë ¥"""
        print("\n" + "="*70)
        print("ğŸ“ˆ G-Eval ì½”ë“œ ìŠ¤ìœ„ì¹­ í‰ê°€ ê²°ê³¼ í†µê³„")
        print("="*70)

        total = len(results)

        # ëª¨ë¸ë³„ í†µê³„
        for model in ['qwen_72b', 'qwen_7b']:
            filtered = [r for r in results if r.model == model]
            if not filtered:
                continue

            count = len(filtered)
            avg_score_a = sum(r.score_a for r in filtered) / count
            avg_score_b = sum(r.score_b for r in filtered) / count

            print(f"ğŸ“Š {model} í†µê³„ (n={count}):")
            print(f"  - Case A í‰ê· : {avg_score_a:.3f}/10.0")
            print(f"  - Case B í‰ê· : {avg_score_b:.3f}/10.0")

        # ì „ì²´ í‰ê·  ì ìˆ˜
        avg_score_a = sum(r.score_a for r in results) / total
        avg_score_b = sum(r.score_b for r in results) / total

        print(f"â­ ì „ì²´ í‰ê·  ì ìˆ˜ (1-10 ì²™ë„):")
        print(f"  - Case A (ì²«ë²ˆì§¸ Case): {avg_score_a:.3f}/10.0")
        print(f"  - Case B (ë‘ë²ˆì§¸ Case): {avg_score_b:.3f}/10.0")

        print(f"ğŸ“Š í‰ê°€ ê¸°ì¤€ë³„ í‰ê·  ì ìˆ˜:")

        avg_helpfulness_a = sum(r.metrics_a['helpfulness'] for r in results) / total
        avg_relevance_a = sum(r.metrics_a['relevance'] for r in results) / total
        avg_accuracy_a = sum(r.metrics_a['accuracy'] for r in results) / total
        avg_depth_a = sum(r.metrics_a['depth'] for r in results) / total

        avg_helpfulness_b = sum(r.metrics_b['helpfulness'] for r in results) / total
        avg_relevance_b = sum(r.metrics_b['relevance'] for r in results) / total
        avg_accuracy_b = sum(r.metrics_b['accuracy'] for r in results) / total
        avg_depth_b = sum(r.metrics_b['depth'] for r in results) / total

        print("  Case A (ì²«ë²ˆì§¸ Case):")
        print(f"    - ìœ ìš©ì„± (Helpfulness): {avg_helpfulness_a:.2f}/10.0")
        print(f"    - ê´€ë ¨ì„± (Relevance): {avg_relevance_a:.2f}/10.0")
        print(f"    - ì •í™•ì„± (Accuracy): {avg_accuracy_a:.2f}/10.0")
        print(f"    - ê¹Šì´ (Depth): {avg_depth_a:.2f}/10.0")
        print("  Case B (ë‘ë²ˆì§¸ Case):")
        print(f"    - ìœ ìš©ì„± (Helpfulness): {avg_helpfulness_b:.2f}/10.0")
        print(f"    - ê´€ë ¨ì„± (Relevance): {avg_relevance_b:.2f}/10.0")
        print(f"    - ì •í™•ì„± (Accuracy): {avg_accuracy_b:.2f}/10.0")
        print(f"    - ê¹Šì´ (Depth): {avg_depth_b:.2f}/10.0")

        print(f"\nğŸ“Š ì¢…í•© ë¶„ì„:")
        print(f"  - ì ìˆ˜ ì°¨ì´: {abs(avg_score_a - avg_score_b):.3f}")
        print("="*70)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="G-Evalì„ ì‚¬ìš©í•œ ì½”ë“œ ìŠ¤ìœ„ì¹­ ì‘ë‹µ í‰ê°€")
    parser.add_argument("--input_file",
                        default="data/outputs/cs/code_switched_response.json.ckpt.json",
                        help="ì…ë ¥ ë°ì´í„° íŒŒì¼")
    parser.add_argument("--output_file",
                        default="data/outputs/cs/g_eval_results.json",
                        help="ì¶œë ¥ ê²°ê³¼ íŒŒì¼")
    parser.add_argument("--model",
                        default="openai/gpt-4o-mini",
                        help="í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë¸")
    parser.add_argument("--workers",
                        type=int,
                        default=3,
                        help="ë©€í‹°ìŠ¤ë ˆë”© ì›Œì»¤ ìˆ˜")
    parser.add_argument("--limit",
                        type=int,
                        default=None,
                        help="í‰ê°€í•  í•­ëª© ìˆ˜ ì œí•œ")
    parser.add_argument("--checkpoint_every",
                        type=int,
                        default=50,
                        help="ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°")
    args = parser.parse_args()

    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = GEvalCodeSwitchingEvaluator(model_name=args.model)

    print(f"ğŸ“Š í‰ê°€ ì„¤ì •:")
    print(f"  - ì…ë ¥ íŒŒì¼: {args.input_file}")
    print(f"  - ì¶œë ¥ íŒŒì¼: {args.output_file}")
    print(f"  - ëª¨ë¸: {args.model}")
    print(f"  - ì›Œì»¤ ìˆ˜: {args.workers}")
    print(f"  - ì²´í¬í¬ì¸íŠ¸ ì£¼ê¸°: {args.checkpoint_every}ê°œë§ˆë‹¤")
    if args.limit:
        print(f"  - í‰ê°€ ì œí•œ: {args.limit}ê°œ í•­ëª©")

    # í‰ê°€ ì‹¤í–‰
    results = evaluator.run_evaluation(
        data_file=args.input_file,
        output_file=args.output_file,
        max_workers=args.workers,
        limit=args.limit,
        checkpoint_every=args.checkpoint_every
    )

    print(f"\nâœ… í‰ê°€ ì™„ë£Œ! ê²°ê³¼ëŠ” {args.output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
